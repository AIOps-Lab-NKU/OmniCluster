[preprocess]
; 待处理数据集名称
to_deal_dataset = OMI
; 待处理数据集存储路径
dataset_path = new-OMI-dataset
; 脚本可处理的数据集列表
all_dataset = ["OMI"]
; 全部数据存储路径，暂时未区分训练集和测试集
save_data_path = new-data/clean-data.npy
; 是否对数据进行切割
if_split = True
; 数据分割窗口大小
split_window_size = 2016
; 数据分割步长
split_stride = 2016
; 是否对根据列表选择数据
if_to_clean = True
; 需要保存的数据index
clean_file = new-data/clean-index.npy
; 是否移除极值
if_remove_extreme = True
; 移除top `extreme_per`%极值
extreme_per = 0.05
;去除极值的方式 extreme or deviation-mean
remove_extreme_mode = extreme
; 是否对数据进行平滑处理
if_moving_average = True
; 平滑处理窗口大小
moving_window_size = 12
; 窗口的最小观测值
min_periods = 1
; 是否进行数据归一化/标准化
if_feature_scaling = True
; norm:归一化 stand：标准化
mode = norm

[train_ae]
; 读取数据路径
data_path = new-data/clean-data.npy
; 记录参数信息
record_path = new-data/result/record.txt
; init autoencoder 模型保存路径
init_ae_model_file = model/init_ae.h5
; 压缩后的z维数据保存路径
init_ae_z_file = new-data/result/init_ae_z.npy
; decoder恢复数据保存路径
init_ae_recons_file = new-data/result/init_ae_reconstruct.npy
; 使用的一维卷积层数
layers_num = 3
; 每层使用的kernel_size
kernel_size_list = [7, 7, 7]
; 每层使用的stride
stride_list = [2, 2, 2]
; 每层使用的output_padding
output_padding_list = [1, 1, 1]
; 每层使用的filters
filters_list = [32, 64, 1]
; dropout比例
dropout_list = [0.2, 0.2, 0.2]
; 激活函数
activation = sigmoid
; 训练前是够对数据进行打乱
if_shuffle = True
; 使用的batch_size大小
batch_size = 256
; 训练轮数
epochs = 100
; 验证集比例
validation_split = 0.1
; 优化器
optimizer = adam
; 损失函数
loss= mse

[yin]
; 每个周期最少包含`win_min`个点
win_min = 1
; 每个周期最多包含`win_max`个点
win_max = 40
; 低于阈值则认为周期性成立
th = 0.1
; 周期性结果写出文件
cycle_path= new-data/result/cycle.npy

[feature_selection]
; 指标周期性阈值，该指标具有周期性的数据数量大于`index_th`时认为该指标为有效指标，目前使用`0.2*数据数量`得到
index_th = 1142
; 指标相似度阈值，两个指标相似度大于`sim_th`时认为两个指标具有相似度
sim_th = 0.95
; 指标相似度阈值，两个指标具有相似度的数据数量大于`sim_num_th`时认为可以去除冗余，目前使用`0.5*数据数量`得到
sim_num_th = 2856
; 经过周期性提取和冗余去除两个步骤后数据保存文件
data_save_path = new-data/result/z_to_cluster.npy

[cal_dis]
; 可以计算的距离度量方式
dis_list = ["PearsonDistance", "Euclidean", "NormEuclidean", "Square", "Manhattan", "SBD"]
; 计算得到的距离矩阵保存路径
distance_dir = new-data/distance

[cluster]
; 可以计算的距离度量方式
dis_list = ["PearsonDistance", "Euclidean", "NormEuclidean", "Square", "Manhattan", "SBD"]
; https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering
; ward, complete, average, single
linkage = ward
; 两条数据间距离大于该值时不能被合并，暂时随便写了一个数
distance_threshold = 5
; 标注文件
label_file = new-data/label.npy
; 结果文件夹
result_dir = new-data/result

